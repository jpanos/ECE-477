<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, available online at https://engineering.purdue.edu/ece477/Course/Policies/policies.html

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
<base href="https://jpanos.github.io/ECE-477/docs/" />

<!--Content-->
<title>ECE477 Course Documents</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="George Hadley">
<meta name = "format-detection" content = "telephone=no" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!--CSS-->
<link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
<link rel="stylesheet" href="css/responsive.css">
<link rel="stylesheet" href="css/styles.css">
<link rel="stylesheet" href="css/content.css">
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
	<!-- Instantiate global site header.-->
	<div id="header"></div>
		<!-- Instantiate site global navigation bar.-->
		<div id="menu"></div>
	
		<!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
			img folder located at this directory level. -->
		<div id="banner">
			<img src="Files/img/BannerImgExample.jpg"></img>
		</div>
	
		<!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
			and include things such as a file lister (for listing out homework assignments or tutorials)
		-->
		<div id="content">
            <h2>Progress Report for Jackie Malayter-</h2>
            
            <h4>Week 1:</h4>
            <b>Date:</b> 9-3-2020<br>
            <b>Total hours:</b>  4.5-5 hours<br>
            <b>Description of design efforts:</b><br>
For week one, I went over the course material and got to know about how 477 will operate this semester. Of course, our group met up this week go over roles and finish our final project proposal. Personally, I started gathering the materials that I needed to start developing how I was going to perform computer vision on the nano.  I found it was not convenient at all to not have internet on my nano, so I purchased a wifi antenna. I also started learning how to interface with the CSI camera included on my nano developer kit, including the pipeline included in the original nano operating system image. This was quite helpful to learn, because it enabled me to start using the camera in python scripts. Additionally, I filled out my sections on the website (except this progress report) and filled out some of the other sections, such as the main page and replacing some images on the team website.</br>


            <br>

            <h4>Week 2:</h4>
            <b>Date:</b> 9-3-2020<br>
            <b>Total hours:</b> 10 <br>
            <b>Description of design efforts:</b><br>
This week, I configured the wifi to my jetson nano. This involved installing a wifi driver on the nano, which was a good learning experience in learning ubuntu (I am not extremely fluent ubuntu). I learned a lot of ubuntu syntax and a lot about remote access- for example, I could ssh into my nano from my personal laptop, as my partner has a static ip address. Nonetheless, having internet connection on the nano itself is very useful for downloading various code repositories, understanding how they operate, and pivoting the code to do something slightly different. I began performing color blob detection on the nano using openCV. I did this with static images- however, I am aiming to perform color blob detection using the live camera feed from gstreamer. This will be an important step for the project, because it will allow us to test if we can use color blob detection in some part of our computer vision process, as it is far less computationally expensive, then say, a CNN (which we might have to use at some point).  </b> 
<br>
<img src = "Team/progress/img/week2jackie.jpg" width="50%" >
<br>
In the image above, that is me working on my nano using a simple facial recognition script I wrote to test out the camera in openCV.  While I was doing this, Moiz and I used the command "tegrastats" to record the power usage of the nano while running this script.  We used those statistics for our project constraints assignment. Moving forward with the nano, I would like to first perform simple live color blob detection. Then, I would like to move onto more advanced image processing. For example, I would like to explore using CNNs for flower identification, as well as starting to explore how to extract positional data from images. This might be difficult, so I am going to work with Ethan on this. 
<br>
Finally, I guided the team on creating a function block diagram of our project. There was a lot of disagreement and unknowns about how our project was going to work, and Josh and I thought it would be extremely helpful to create a high level functional diagram of exactly how our drone was going to progress through its operation. This ended up being important for our constraints homework, because making a block diagram as a team got everyone on the same page of what our end product was going to do. Therefore, it was much easier to think about constraints and discuss them when we were all on the same page. Below is a picture of the diagram.  
<br>
<img src = "Team/progress/img/week2flow.png" width="80%">
<br>

	<h4>Week 3:</h4>
	<b>Date:</b> 9-11-2020<br>
	<b>Total hours:</b> 15 or so <br>
	<b> Description of design efforts:</b><br>
On Saturday, I helped Moiz plan on how to mount the hardware on the drone. This required me to desolder the ESCs on the drone because they were in the way of the pixhawk. Extra care had to be taken when desoldering because we did not want to destroy the power distribution board by overheating the components. Additionally, I took apart the drone so that we could make an entrance for the battery to fit into in the bottom of the drone. Below is a picture of me doing just that.
<br>
<img src = "Team/progress/img/screwDrone.jpg" width="70%">
<br>
This week, our homework was to analyze the components of our project.  I analyzed the Jetson Nano and the STM32H7, as I wanted to learn more in depth about the features of these components. I already knew quite a bit about the Nano, because I have been working with it since the beginning of the semester. However, I also analyzed an Analog Devices BlackFin DSP chip that we were considering using this summer to offboard the image processing from the STM32H7. The Nano was definitely the superior choice for prototyping, because, among other things, it came with a dev kit and has a huge presence of documentation on the web. This isn't to say that the BlackFin DSP chip is "useless"- I could definitely see it being used on perhaps a more refined design- that is, we know <i>exactly</i> what needs to happen computationally, and we are simply seeking to reduce power consumption. In such a case, the nano definitely could be replaces with a less power hungry chip, which could be a BlackFin DSP processor. Additionally, I compared the STM32H7 with the STM32H4.  From this, I grasped a better understanding of all the peripherals and features of the STM32H7, which was useful to me, because we just decided to use this micro during the summer and I didn't look too deep into it after that. 
<br>
Finally, in between our group meetings and working with the drones, I started my color blob detection. I don't have anything interesting to show quite yet because I was working on finding a good IDE and have been debugging my code. Right now, I am using openCV's Simple Blob Detector. I am using greyscale images, which is not ideal, but I want to explore how I can incorporate color. Here is a picture of my pycharm IDE and an example of a flower picture with very rough blob detection (as you can see, my algorithm obviously needs more refinement, but it is a first step!). As you can wee, the blob detection is selecting the stamen of the flowers, but it is also picking out other things. This requires more work to isolte the stamen only. 
<br>
<!-- picture of ide and flower-->
<img src = "Team/progress/img/pycharm.png" width = "80%">
<img src = "Team/progress/img/colorblobWeek3.png" width = "70%">
<br>

<h4> Week 4:</h4>
<b>Date:</b> 9-17-2020 <br>
<b>Total Hours: </b> 15 or so<br>
<b> Description of design efforts:</b><br>
	This week, I refined the image processing algorithm for detecting flowers and got it working with live camera feed on the nano.  I will talk about how I did it. First of all, we designed our flower detection plan such that when we were in close range of a plant, we would use a faster algorithm than a neural net.  It's my primary responsibility to make this faster, more simple algorithm.  Assuming that we are already near a flower, I could use a less precise method than a neural net.  Thus, I am using color masking and blob detection in openCV. <br>
	I created a small data set of 14 images of blossoms to test out how I was going to create this algorithm. I tried all sorts of things, such as blob detection, masking out objects that weren't white (to isolate just the petals), and masking out objects that weren't yellow (to isolate the stamen only), but after I did these, I wasn't quite sure how to label the individual flowers. I then decided to keep it as simple as I could, while still getting a decent result. Below is a process of how the stamen are detected on the nano:<br>
1. Instantiate openCV simple blob detector and write gsstreamer command for CSI camera <br>
2. Begin video capture (and exit on esc key)<br>
3. For each frame, convert color to hsv colorspace and create a color mask for any colors in the range ([14,64,120]) to ([30,255,255])<br>
4. Invert color mask so that stamen blobs are dark<br>
5. Feed inverted mask into the simple blob detector, which detects groups of dark pixels <br>
6. Draw detected blobs on frame and display to user (of course, this step won't be needed on the drone, but is necessary for verification)<br>
<br> 
Here is the picture of the mask and final product: <br>
<img src = "Team/progress/img/blobDetect.png" width = "100%"> <br>
On the left, you see the mask that openCV generated that the blob detection is being performed on, and the subsequent frame. <br>

        </div>
<h4> Week 5 </h4>	<br>
<b> Date:  </b> 9/25/2020 <br>
<b> Total Hours: </b> 13 or so<br>
<b> Description of Design Efforts:</b> <br> 
This week was not as productive as I would have hoped for. After successfully identifying flowers in the frame, it was time to start trying to use a stereo camera rig to identify the distance of flowers. Ethan and I crafted a stereo rig prototype on a piece of particle board. I successfully got my algorithm from last week working on both cameras at the same time.  It seems rather slow, however, upon further investigation, I think it actually is slowed by the display of the video (specifically- the waitkey command is lagging the stream), which will not be a problem in the final product. Of course, more testing is necessary to verify this claim, and I must still keep designing my code to be as robust as possible. 
I spend a lot of time trying some depth map algorithms and tyring to see if I could just compute the parallax of the identified flowers from last week. The depth map I tried worked very poorly, and using the identified flower locations from last week's algorithm has some issues. First of all, it is extremely difficult to correlate which point corresponds to the same flower between frames, espectially because one camera might miss a flower or 2. One of the reasons this is difficult is because of camera distortion, and, although I wanted to get away with not calibrating the cameras, I finally decided I should take this step. So, currently I am going to calibrate the cameras using a checkerboard to detect and reverse the distortion. Then, I will try using a fast feature extractor to make a better depth map and estimate the distance of the flowers. Initially, this algorithm may be slow, but I just need to get something working and then I can work on optimizing it later. Below are some of the calibration pictures I took. As you can see, the distortion is not terrible. 

</div>
<img src = "Team/progress/img/_img1_334.jpg" width = "70%" style="transform: scaleY(-1);"> <br>
<img src = "Team/progress/img/_img1_240.jpg" width = "70%" style = "transform: scaleY(-1);"> <br>

<h4> Week 6 </h4> <br>
<b> Date: </b> <br>
<b> Total Hours: </b> 13 or so <br>
<b> Description of Design Efforts: </b> <br>
This week, I began the calibration script of the two cameras and began extracting the camera calibration data using opencv. This calibration is a one-time thing, so it only needs to be performed once and then the "camera matrices" can be stored for future use on the jetson nano.  <br> I have been working a lot on the image processing lately, and wanted to work more on hardware for a little bit.  I am going to start working on I2C for the battery moniter peripheral. To prepare for this, I soldered the battery moniter on a development board. This was the first time I had ever soldered something with such tiny pins on a flat surface. I first practiced on an old ECE 362 board and surface mount capacitor to make sure that I was prepared.  Below you can see some pictures of me soldering the actual battery moniter integrated circuit to the breakout board. <br> Additionally, I soldered additional pins onto my stm32h7 nucleo board. I have included some pictures of my soldering job, which isn't too terrible for maybe the third time I have soldered in my life. After that, I got the Cube IDE for my stm32h7 and will begin working on an I2C functionality between the battery moniter and the stm32h7.

<br>
<img src = "Team/progress/img/soldering.jpeg" width = "30%" > <br>
<img src = "Team/progress/img/stmsolder.jpg" width = "30%"> <br>
<h4> Week 7 </h4>
<b> Total Hours </b> 15 <br>
<b> Description of Design Efforts: </b> <br>
This week, I worked on the software formalization and I2C. In the software formalization, I read through and interpreted the liscening policies on using third party softwares. I was very surprised to see that openCV had patented algorithms in its open source release (SIFT/SURF algorithms).  It is a good thing we had to pay attention to this or else we could be in legal trouble if we sold our product in the future. Most of the softare formaliation was going more in detail of the functional description of software components, which was right up my alley as a systems engineering. It was enjoyable to describe how the whole drone was going to work as one unit. 
Additionally, I worked on I2C this week. We are using I2C to communication with the battery monitor. To be honest, we learned I2C in ECE362 during the peak of the covid-19 shutdowns and therefore I didn't learn it very well. So, I spent a lot of time this week learning about how I2C works. Luckily, STM's HAL (hardware abstraction libraries) do a lot of the work for you, but, nonetheless, I wanted to understand what exactly was happening. Here is something interesting I learned about I2C this week. So, the default clock to the I2C peripheral is 48 MHz, but the battery monitor can only support ~32kHz for I2C.  However, I was really confused because the prescaler maxed out at 16, so I was very curious about how we were going to make this work.  It turns out that the SCLL and SCLH bits in the timing register can be used to extend the clock period to yield a slower rate clock. I have show in diagrams below how the clock is slowed to the desired value. I should be finishing up I2C tomorrow.
<br>
<img src = "Team/progress/img/scll.jpg" width = 50% > <br>
<img src = "Team/progress/img/sclh.jpg" width = 50%> <br>
<br>
<h4> Week 8 </h4> 
<b> Total Hours </b> 15 <br>
<b> Description of Design Efforts: </b> <br>
This week, I continued working with the battery monitor, as well as helping generate a clock for our battery monitor using pulse width modulation. I configured I2C2 on our device to send the start bit and send the slave address, as well as request a write to the device. Then, I wanted to actually test this on the monitor chip. Unfortunately, upon being very skeptical that our battery monitor did not have an internal oscillator, we realized that our battery monitor is actually the analog front end to another battery monitoring chip (that somehow was not mentioned at all in the data sheet...). So, I could not test my I2C and try to get an ACK from the slave. Also, we had to select a new chip. Once we selected the new chip, the bq76920, I started working on more modular functions to write and read to the register addresses provided in the data sheet. What I have to do now, finally, is to create 2 ISRs - one for i2c2txne (transmitter register not empty) and i2c2rxne (recieve register not empty), so that we can actually transmit and recieve bytes via I2C. I am currently inthe process of writing those ISRs, and once the chip is shipped, I hopefully will be able to test and debug them. Below, you can see my 100 kHz I2C clock, sending the slave address 0x8, and requesting a write. Because I2C is not connected to any device, we see a NACK. 
<br>
<br>
<br>
<img src = "Team/progress/img/i2c2.png" width = "50%" style= "transform: rotate(90deg)"> <br>
<br><br><br>
<h4> Week 9 </h4>
<b> Total Hours </b> 15 <br>
<b> Description of Design Efforts: </b> <br>
This week, I kept working on I2C. Last week, I got the I2C2 to talk to I2C1. This was to prepare for communication with the new battery monitor that we ordered. First, I initiated a read request from I2C1 (I2C2 is the master device).  I successfully recieved data from the I2C1 slave on I2C2 using my interrupt service routines. Next, I wrote an interrupt service routine to write data to I2C1.  Because the battery monitor requires its address to be sent followed by the destination register address, I had to write my ISR to send the two addresses and then send the desired byte. This was fairly simple; the process involves first sending a start bit with the slave address. Then, I enable the TXE interrupt on I2C2 after sending the start bit so that an interrupt is triggered. In the first TXE flag event, I place the desired register address into the transmit data register of the I2C2 peripheral.  The second flag of TXE, I place the desired byte that I want to write to the battery monitor.  The battery monitor really only supports byte writes, therefore this is sufficient functionality.  Moving forward, I will test connection to the actual battery monitor and not I2C1 and debug from there. Otherwise, this PSSC is pretty much done and will require just some formalization. 
<br>
Below: Picture of the transmitter sending 0xAA after sending the desired slave address.
<br>
<img src = "Team/progress/img/jackieweek9.jpg" width="50%"> <br> 
<br>

<h4> Week 10 </h4>
<b> Total Hours </b> 15 <br>
<b> Description of Design Efforts; </b> <br>
This week I soldered the battery monitor onto a dev board so that I could test it with my I2C code (I also learned how to desolder an IC in this process, which was a cool learning experience).  I first changed the address to the bq76920's address 0x08 and attempted to write to the battery monitor. I recieved an ack from the monitor and the byte I was writing appeared on teh oscilloscope. However, I couldn't be too sure that I was actually writing to the battery monitor. So, to check, I wrote to a register 0x18 and then read the value at that address. I successfully verified what I wrote indeed was written to the battery monitor. The rest of the week I spent my time formalizing the process for getting data from the battery monitor.  I am current debugging this because I thought that the circuit was capable of block reads, but I can only read a total of 2 bytes maximum.  This is leading me to believe that the IC only supports 2 byte block reads because each of the registers has a high and a low address, which would make reading 2 bytes at a time much more logical. Unfortunately, when I try to read from some select registers, I am not able to read data from them, even though the datasheet says I should be able to.  I am currently debugging that and will be able to read from the desired registers of the battery monitor in increments of 250 ms, which is the update rate of the cell voltages. 
<br>
Below: Picture of my battery monitor test setup
<br>
<img src = "Team/progress/img/battmonit.jpg" width="50%"> <br>
 

<br>
<h4> Week 11 </h4>
<b> Total Hours </b> 20 <br>
<b> Description of Design Efforts: </b> <br>
Work continued this week for the battery monitor. I began programming the regular interrupt to read the 4 cell registers and the total battery register values. This was interestin because the timer I was using for the interrupt was on the M4 core of our microcontroller, but my code and my ISR was on the M7 core of the microcontroller. Because my original I2C code I had been working on contained global variables on the M7, I needed to figure out how to have shared global variabled between two cores. Moiz showed me how to use shared memory between two cores, which is essentially multithreading.  Luckily, my multithreading process was very simple, because it was guaranteed that the two cores would not access the shared memory at the same time. So, there was no need for any sort of 'locks' on the shared memory, which simplified things greatly. <br>
On my development board, I successfully read values from the battery monitor; however, they were not accurate because the maximum voltage the monitor was powered at was 5V which was too low for the chip's
ADC to work.  Therefore, I wired up the battery monitor to the PCB (I wired one of the PCBs for testing the monitor, while Josh wired a second). This time, I was getting more realistic voltages but they still did not seem 100% accurate.  I am currently debugging why I am getting slightly strange battery monitor reads. <br> On top of my battery monitor progress, I spent some time testing the drone flight with Moiz and Josh. <br>
Below: Picture of the battery monitor on the PCB<br>
<img src = "Team/progress/img/pcbmonitor.jpg" width="50%"> <br>
<br>
<h4> Week 12 </h4>
<b> Total Hours </b> 18 <br>
<b> Description of Design Efforts: </b> <br>
This week I did two things. First, I helped build a drone cage with Moiz. Second, I finally succeeded in reading accurate battery values from the batter monitoring integrated circuit.  The drone cage was a necessity because there is a bug with MavLink causing the drone to flip itself over and crash into the ground. Nearly every test, propellors are usually broken, and we are on a limited supply. Therefore, we got some PVC pipe and pipe connectors and glued them together to create a protective cage around the drone. Although this is not perfect, it does protect some of the hardware and more propellors than without it.  We are refining the bubble for further testing.
Fixing the bug in the battery monitor was a rather arduous process. There was not really one major thing that was flawed with the software, but very small things that would cause failure.  For example, I casted a register as a uint32_t, but tried to pass in a negative value which was used for a special condition. As well, I forgot to clear the 'size' register in the I2C starting sequence, meaning that it was always requesting a read of 2 bits, while some of the reads I wanted to perform only required 1 bit. This issue impacted the interrupt because, if I was doing a 1 bit read or write, then I would accidentlly trigger an additional transmitter empty or reciever not empty interrupt.  This caused unexpected transmissions on the oscilloscope. However, once I fixed that bug, my interrupt works great and I am reading the actual battery voltage from the chip with a close degree of accuracy.  The next step is to read individual cells to predict the remaining battery life. Below is a video of my test. 
<br> Measuring battery voltage with the battery monitor <br>
<iframe src="https://drive.google.com/file/d/1m7Io_aJz-GeNZ2mpK6-Qaq1KyZdUTfPo/preview" width="640" height="480"></iframe>


		<!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
		<div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $("#header").load("header.html");
	$("#menu").load("navbar.html");
	$("#footer").load("footer.html");
});
</script>
</body>
</html>
